# Chat Orchestrator & Tool Routing (Task 3)

**Implementation Date:** December 11, 2025  
**Status:** âœ… Complete - All 17 tests passing  
**Related Documentation:** 
- [Agent & LLM Tracking](./AGENT_LLM_TRACKING.md)
- [Provider Abstraction](./PROVIDER_ABSTRACTION.md)

## Table of Contents
1. [Overview](#overview)
2. [Architecture](#architecture)
3. [Components](#components)
4. [Implementation Details](#implementation-details)
5. [Tool Routing System](#tool-routing-system)
6. [API Changes](#api-changes)
7. [Testing](#testing)
8. [Usage Examples](#usage-examples)
9. [Future Enhancements](#future-enhancements)

---

## Overview

Task 3 introduces a central **ChatOrchestrator** that coordinates the complete chat interaction flow, including:

- **Agent and LLM resolution** - Determines which agent and model to use
- **Provider routing** - Sends messages to appropriate LLM providers
- **Tool execution** - Processes tool calls requested by LLMs (Protegrity tools)
- **Message persistence** - Saves assistant messages with proper tracking

### Goals Achieved

âœ… Central orchestration layer for chat flow  
âœ… Tool routing with permission validation  
âœ… Protegrity tool integration (redact, classify, guardrails)  
âœ… Refactored `/api/chat/` and `/api/chat/poll/` to use orchestrator  
âœ… Extended `ProviderResult` to support tool calls  
âœ… DummyProvider simulates tool calls for development  
âœ… Comprehensive test coverage (17 tests)

---

## Architecture

### High-Level Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         API Endpoint                             â”‚
â”‚                    (/api/chat/ POST)                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     ChatOrchestrator                             â”‚
â”‚  â€¢ Resolve agent & LLM                                          â”‚
â”‚  â€¢ Route to provider                                            â”‚
â”‚  â€¢ Execute tool calls                                           â”‚
â”‚  â€¢ Persist messages                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚                         â”‚
             â–¼                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   LLM Provider         â”‚   â”‚    Tool Router           â”‚
â”‚  (DummyProvider, etc)  â”‚   â”‚  â€¢ Validate permissions  â”‚
â”‚  â€¢ Send message        â”‚   â”‚  â€¢ Execute tools         â”‚
â”‚  â€¢ Return tool_calls   â”‚   â”‚  â€¢ Return results        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                          â”‚
                                          â–¼
                             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                             â”‚  Protegrity Service        â”‚
                             â”‚  â€¢ redact_data()          â”‚
                             â”‚  â€¢ discover_entities()    â”‚
                             â”‚  â€¢ check_guardrails()     â”‚
                             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow

1. **User Message** arrives via API
2. **Orchestrator** resolves agent/LLM from conversation
3. **Provider** processes message and optionally requests tools
4. **Tool Router** validates permissions and executes tools
5. **Orchestrator** creates assistant message with results
6. **API** returns response with messages and tool results

---

## Components

### 1. ProviderResult (Extended)

**File:** `backend/apps/core/providers.py`

Extended to support tool call requests from LLMs:

```python
class ProviderResult:
    """
    Standard result object for provider calls.
    
    Attributes:
        status: "completed" | "pending"
        content: Assistant response text (when completed)
        pending_message_id: Message ID for async providers (optional)
        tool_calls: List of tool call requests from the LLM
                   Each tool call is a dict with:
                   {
                       "tool_name": "protegrity-redact",
                       "arguments": {...},
                       "call_id": "tool_call_1",
                   }
    """
    
    def __init__(self, status, content=None, pending_message_id=None, tool_calls=None):
        self.status = status
        self.content = content
        self.pending_message_id = pending_message_id
        self.tool_calls = tool_calls or []
```

**Changes:**
- Added `tool_calls` parameter (default: empty list)
- Updated `__repr__` to show tool call count
- Tool calls use standardized format across all providers

### 2. DummyProvider (Tool Simulation)

**File:** `backend/apps/core/providers.py`

Updated to simulate tool calls based on keyword detection:

```python
class DummyProvider(BaseLLMProvider):
    def send_message(self, conversation, messages, agent=None):
        # ... existing code ...
        
        user_text_lower = user_text.lower()
        tool_calls = []
        
        # Detect patterns and trigger appropriate tools
        if "ssn" in user_text_lower or "social security" in user_text_lower:
            tool_calls.append({
                "tool_name": "protegrity-redact",
                "arguments": {"text": user_text},
                "call_id": "tool_call_1",
            })
        
        if "classify" in user_text_lower or "find pii" in user_text_lower:
            tool_calls.append({
                "tool_name": "protegrity-classify",
                "arguments": {"text": user_text},
                "call_id": f"tool_call_{len(tool_calls) + 1}",
            })
        
        if "guardrail" in user_text_lower or "check policy" in user_text_lower:
            tool_calls.append({
                "tool_name": "protegrity-guardrails",
                "arguments": {"text": user_text},
                "call_id": f"tool_call_{len(tool_calls) + 1}",
            })
        
        return ProviderResult(
            status="completed",
            content=reply_text,
            tool_calls=tool_calls,
        )
```

**Trigger Keywords:**
- `"ssn"`, `"social security"` â†’ **protegrity-redact**
- `"classify"`, `"find pii"`, `"discover"` â†’ **protegrity-classify**
- `"guardrail"`, `"check policy"`, `"validate"` â†’ **protegrity-guardrails**

### 3. Tool Router

**File:** `backend/apps/core/tool_router.py` (NEW)

Handles tool execution with permission validation:

```python
def execute_tool_calls(agent, tool_calls: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    Execute tool calls for a given agent, validating permissions.
    
    Only tools assigned to the agent and marked as active will be executed.
    """
    if not tool_calls:
        return []
    
    # Pre-fetch agent tools for quick lookup
    agent_tools = {t.id: t for t in agent.tools.all()} if agent else {}
    
    results = []
    
    for call in tool_calls:
        tool_name = call.get("tool_name")
        call_id = call.get("call_id")
        args = call.get("arguments", {})
        
        # Validate tool exists and agent has permission
        tool = agent_tools.get(tool_name)
        if not tool or not tool.is_active:
            results.append({
                "call_id": call_id,
                "tool_name": tool_name,
                "error": "Tool not allowed or not found for this agent.",
            })
            continue
        
        # Dispatch based on tool type
        try:
            if tool.tool_type == "protegrity":
                output = _execute_protegrity_tool(tool, args)
            else:
                output = {"warning": "Tool type not implemented yet."}
            
            results.append({
                "call_id": call_id,
                "tool_name": tool_name,
                "output": output,
            })
        except Exception as exc:
            results.append({
                "call_id": call_id,
                "tool_name": tool_name,
                "error": str(exc),
            })
    
    return results
```

**Protegrity Tool Mappings:**

```python
def _execute_protegrity_tool(tool: Tool, args: Dict[str, Any]) -> Dict[str, Any]:
    protegrity = get_protegrity_service()
    text = args.get("text", "")
    
    if tool.id == "protegrity-redact":
        redacted_text, metadata = protegrity.redact_data(text)
        return {
            "redacted_text": redacted_text,
            "original_length": len(text),
            "redacted_length": len(redacted_text),
            "metadata": metadata,
        }
    
    elif tool.id == "protegrity-classify":
        entities = protegrity.discover_entities(text)
        return {
            "entities": entities,
            "entity_types": list(entities.keys()),
            "total_entities": sum(len(v) for v in entities.values()),
            "original_text": text,
        }
    
    elif tool.id == "protegrity-guardrails":
        return protegrity.check_guardrails(text)
    
    elif tool.id == "protegrity-protect":
        protected_text, metadata = protegrity.protect_data(text)
        return {
            "protected_text": protected_text,
            "success": protected_text is not None,
            "metadata": metadata,
        }
    
    elif tool.id == "protegrity-unprotect":
        unprotected_text, metadata = protegrity.unprotect_data(text)
        return {
            "unprotected_text": unprotected_text,
            "success": unprotected_text is not None,
            "metadata": metadata,
        }
```

**Security Features:**
- âœ… Agent must have tool assigned via M2M relationship
- âœ… Tool must be marked `is_active=True`
- âœ… Unauthorized tools return error instead of executing
- âœ… Disabled tools return error
- âœ… Exceptions caught and returned as errors

### 4. ChatOrchestrator

**File:** `backend/apps/core/orchestrator.py` (NEW)

Central coordination layer for chat interactions:

```python
class ChatOrchestrator:
    """
    Orchestrates the complete flow of a chat interaction.
    """
    
    def _resolve_agent_and_llm(self, conversation: Conversation):
        """
        Determine which agent and LLM to use for this conversation.
        
        Resolution logic:
        1. Use conversation.primary_agent
        2. Use conversation.primary_llm
        3. If primary_llm is None, fallback to agent.default_llm
        4. Update conversation if fallback was used
        """
        agent = conversation.primary_agent
        llm = conversation.primary_llm
        
        if llm is None and agent and agent.default_llm:
            llm = agent.default_llm
            conversation.primary_llm = llm
            conversation.model_id = llm.id
            conversation.save(update_fields=["primary_llm", "model_id"])
        
        return agent, llm
    
    @transaction.atomic
    def handle_user_message(self, conversation, user_message):
        """
        Process a new user message through the complete chat pipeline.
        
        Steps:
        1. Resolve agent and LLM
        2. Get conversation history
        3. Send to LLM provider
        4. Execute any requested tool calls
        5. Create assistant message with results
        """
        agent, llm = self._resolve_agent_and_llm(conversation)
        
        if not llm:
            # Create error message
            error_msg = Message.objects.create(
                conversation=conversation,
                role="assistant",
                content="Error: No LLM provider configured.",
                pending=False,
                blocked=False,
                agent=agent,
                llm_provider=None,
            )
            return {
                "assistant_message": error_msg,
                "tool_results": [],
                "status": "error"
            }
        
        provider = get_provider(llm)
        history = conversation.messages.order_by("created_at")
        result = provider.send_message(conversation, history, agent=agent)
        
        tool_results = []
        assistant_msg = None
        
        if result.status == "completed":
            # Execute tool calls if requested
            if result.tool_calls:
                tool_results = execute_tool_calls(agent, result.tool_calls)
            
            # Prepare content with tool summary
            content = result.content or ""
            if tool_results:
                tool_summary = "\n\n---\n**Tools Used:**\n"
                for tr in tool_results:
                    if "error" in tr:
                        tool_summary += f"- âŒ {tr['tool_name']}: {tr['error']}\n"
                    else:
                        tool_summary += f"- âœ… {tr['tool_name']}: Success\n"
                content += tool_summary
            
            # Create assistant message
            assistant_msg = Message.objects.create(
                conversation=conversation,
                role="assistant",
                content=content,
                protegrity_data={"tool_results": tool_results} if tool_results else None,
                pending=False,
                blocked=False,
                agent=agent,
                llm_provider=llm,
            )
        
        elif result.status == "pending":
            assistant_msg = Message.objects.create(
                conversation=conversation,
                role="assistant",
                content="",
                pending=True,
                blocked=False,
                agent=agent,
                llm_provider=llm,
            )
        
        return {
            "assistant_message": assistant_msg,
            "tool_results": tool_results,
            "status": result.status,
        }
    
    def poll(self, conversation):
        """
        Poll for async provider results for a conversation.
        """
        agent, llm = self._resolve_agent_and_llm(conversation)
        
        if not llm:
            return {"status": "error", "assistant_message": None, "tool_results": []}
        
        provider = get_provider(llm)
        result = provider.poll_response(conversation)
        
        if result is None or result.status == "pending":
            return {"status": "pending", "assistant_message": None, "tool_results": []}
        
        # Process completed response (similar to handle_user_message)
        tool_results = []
        if result.tool_calls:
            tool_results = execute_tool_calls(agent, result.tool_calls)
        
        content = result.content or ""
        if tool_results:
            tool_summary = "\n\n---\n**Tools Used:**\n"
            for tr in tool_results:
                if "error" in tr:
                    tool_summary += f"- âŒ {tr['tool_name']}: {tr['error']}\n"
                else:
                    tool_summary += f"- âœ… {tr['tool_name']}: Success\n"
            content += tool_summary
        
        assistant_msg = Message.objects.create(
            conversation=conversation,
            role="assistant",
            content=content,
            protegrity_data={"tool_results": tool_results} if tool_results else None,
            pending=False,
            blocked=False,
            agent=agent,
            llm_provider=llm,
        )
        
        return {
            "status": "completed",
            "assistant_message": assistant_msg,
            "tool_results": tool_results,
        }
```

**Key Features:**
- âœ… Single responsibility: coordinate chat flow
- âœ… Transaction-safe message creation
- âœ… Automatic LLM fallback to agent default
- âœ… Tool results stored in `protegrity_data` JSON field
- âœ… Tool summary appended to assistant message
- âœ… Proper error handling for missing LLM

---

## Implementation Details

### Tool Call Format

**From LLM Provider â†’ Orchestrator:**
```python
{
    "tool_name": "protegrity-redact",    # Must match Tool.id in database
    "arguments": {                       # JSON-serializable parameters
        "text": "SSN: 123-45-6789"
    },
    "call_id": "tool_call_1"            # Unique identifier for this call
}
```

**From Tool Router â†’ Orchestrator:**
```python
# Success
{
    "call_id": "tool_call_1",
    "tool_name": "protegrity-redact",
    "output": {
        "redacted_text": "SSN: [SSN]",
        "metadata": {...}
    }
}

# Error
{
    "call_id": "tool_call_1",
    "tool_name": "protegrity-redact",
    "error": "Tool not allowed or not found for this agent."
}
```

### Message Tracking

All assistant messages created by the orchestrator include:

```python
Message.objects.create(
    conversation=conversation,
    role="assistant",
    content=content,
    agent=agent,                          # âœ… Agent tracking
    llm_provider=llm,                     # âœ… LLM tracking
    protegrity_data={                     # âœ… Tool results stored
        "tool_results": tool_results
    },
    pending=False,
    blocked=False,
)
```

### Database Schema

No new migrations required - uses existing fields:

- `Message.agent` - ForeignKey to Agent
- `Message.llm_provider` - ForeignKey to LLMProvider  
- `Message.protegrity_data` - JSONField for tool results
- `Agent.tools` - ManyToMany to Tool (for permissions)
- `Tool.is_active` - BooleanField (for enabling/disabling)

---

## API Changes

### POST /api/chat/

**Before (Task 2):**
```python
# Inline provider logic
provider = get_provider(llm)
result = provider.send_message(conversation, history, agent=agent)

if result.status == "completed":
    assistant_msg = Message.objects.create(...)
```

**After (Task 3):**
```python
# Use orchestrator
orchestrator = ChatOrchestrator()
result = orchestrator.handle_user_message(conversation, user_message)

assistant_msg = result.get("assistant_message")
tool_results = result.get("tool_results", [])
status = result.get("status")
```

**Response Format (New):**
```json
{
  "conversation_id": "uuid",
  "status": "completed",
  "messages": [
    {"role": "user", "content": "My SSN is 123-45-6789"},
    {
      "role": "assistant",
      "content": "I detected sensitive data...\n\n---\n**Tools Used:**\n- âœ… protegrity-redact: Success",
      "agent": "Data Protection Expert",
      "llm": "Dummy LLM"
    }
  ],
  "tool_results": [
    {
      "call_id": "tool_call_1",
      "tool_name": "protegrity-redact",
      "output": {
        "redacted_text": "My SSN is [SSN]",
        "metadata": {...}
      }
    }
  ],
  "protegrity_data": {...}
}
```

### GET /api/chat/poll/{conversation_id}/

**Before (Task 2):**
```python
# Poll provider directly
provider = get_provider(llm)
result = provider.poll_response(conversation)

if result.status == "completed":
    # Manually update message
    pending_message.content = result.content
    pending_message.save()
```

**After (Task 3):**
```python
# Use orchestrator
orchestrator = ChatOrchestrator()
result = orchestrator.poll(conversation)

assistant_msg = result.get("assistant_message")
tool_results = result.get("tool_results", [])
```

**Response Format (New):**
```json
{
  "status": "completed",
  "response": "LLM response with tool results...",
  "tool_results": [...],
  "protegrity_output": {...},
  "agent": "Data Protection Expert",
  "llm": "Dummy LLM"
}
```

---

## Tool Routing System

### Permission Model

```
Agent.tools (ManyToMany) â”€â”€â”€â”€â”€â–º Tool
   â”‚                              â”‚
   â”‚                              â”œâ”€ id (PK)
   â”‚                              â”œâ”€ tool_type
   â”‚                              â”œâ”€ is_active
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€ function_schema
```

**Example Setup:**
```python
# Create tools
redact_tool = Tool.objects.create(
    id="protegrity-redact",
    name="Protegrity Data Redaction",
    tool_type="protegrity",
    is_active=True
)

classify_tool = Tool.objects.create(
    id="protegrity-classify",
    name="Protegrity PII Discovery",
    tool_type="protegrity",
    is_active=True
)

# Assign to agent
agent = Agent.objects.get(id="data-protection-expert")
agent.tools.add(redact_tool, classify_tool)
```

### Execution Flow

```
LLM requests tool
       â”‚
       â–¼
Tool Router validates
   - Agent has tool?
   - Tool is active?
       â”‚
       â”œâ”€ No â”€â”€â–º Return error
       â”‚
       â–¼ Yes
Execute tool
   - Call Protegrity SDK
   - Catch exceptions
       â”‚
       â–¼
Return result or error
```

### Security Guarantees

1. **Agent Authorization**: Only tools assigned to agent can execute
2. **Active Status**: Disabled tools (`is_active=False`) return error
3. **Type Validation**: Tool type must match handler
4. **Exception Safety**: All tool errors caught and returned
5. **Permission Isolation**: No agent = no tool execution

---

## Testing

### Test Coverage

**File:** `backend/apps/core/tests/test_orchestrator.py`

**Total Tests:** 17 (all passing âœ…)

#### TestToolRouter (7 tests)
- `test_execute_tool_calls_empty_list` - Empty list returns empty results
- `test_execute_tool_calls_no_agent` - No agent returns authorization errors
- `test_execute_tool_calls_unauthorized_tool` - Agent without tool access gets error
- `test_execute_tool_calls_disabled_tool` - Disabled tools return error
- `test_execute_tool_calls_redact_success` - Redact tool executes successfully
- `test_execute_tool_calls_classify_success` - Classify tool executes successfully
- `test_execute_multiple_tool_calls` - Multiple tools execute in sequence

#### TestChatOrchestrator (5 tests)
- `test_resolve_agent_and_llm` - Resolves agent/LLM from conversation
- `test_resolve_fallback_to_agent_default_llm` - Falls back to agent default
- `test_handle_user_message_creates_assistant_message` - Creates message with tracking
- `test_handle_user_message_with_tool_calls` - Executes tools when triggered
- `test_handle_user_message_no_llm_error` - Returns error when no LLM

#### TestChatAPIWithOrchestrator (5 tests)
- `test_chat_endpoint_creates_conversation_and_messages` - End-to-end chat flow
- `test_chat_endpoint_with_tool_trigger` - SSN triggers redact tool
- `test_chat_endpoint_with_classify_trigger` - Classify keyword triggers tool
- `test_chat_endpoint_tracks_agent_and_llm` - Tracking persisted correctly
- `test_chat_endpoint_existing_conversation` - Reuses existing conversation

### Running Tests

```bash
# Run all orchestrator tests
python -m pytest apps/core/tests/test_orchestrator.py -v

# Run specific test class
python -m pytest apps/core/tests/test_orchestrator.py::TestToolRouter -v

# Run with coverage
python -m pytest apps/core/tests/test_orchestrator.py --cov=apps.core --cov-report=html
```

### Test Results

```
====================== test session starts ======================
collected 17 items

apps/core/tests/test_orchestrator.py::TestToolRouter::test_execute_multiple_tool_calls PASSED [  5%]
apps/core/tests/test_orchestrator.py::TestToolRouter::test_execute_tool_calls_classify_success PASSED [ 11%]
apps/core/tests/test_orchestrator.py::TestToolRouter::test_execute_tool_calls_disabled_tool PASSED [ 17%]
apps/core/tests/test_orchestrator.py::TestToolRouter::test_execute_tool_calls_empty_list PASSED [ 23%]
apps/core/tests/test_orchestrator.py::TestToolRouter::test_execute_tool_calls_no_agent PASSED [ 29%]
apps/core/tests/test_orchestrator.py::TestToolRouter::test_execute_tool_calls_redact_success PASSED [ 35%]
apps/core/tests/test_orchestrator.py::TestToolRouter::test_execute_tool_calls_unauthorized_tool PASSED [ 41%]
apps/core/tests/test_orchestrator.py::TestChatOrchestrator::test_handle_user_message_creates_assistant_message PASSED [ 47%]
apps/core/tests/test_orchestrator.py::TestChatOrchestrator::test_handle_user_message_no_llm_error PASSED [ 52%]
apps/core/tests/test_orchestrator.py::TestChatOrchestrator::test_handle_user_message_with_tool_calls PASSED [ 58%]
apps/core/tests/test_orchestrator.py::TestChatOrchestrator::test_resolve_agent_and_llm PASSED [ 64%]
apps/core/tests/test_orchestrator.py::TestChatOrchestrator::test_resolve_fallback_to_agent_default_llm PASSED [ 70%]
apps/core/tests/test_orchestrator.py::TestChatAPIWithOrchestrator::test_chat_endpoint_creates_conversation_and_messages PASSED [ 76%]
apps/core/tests/test_orchestrator.py::TestChatAPIWithOrchestrator::test_chat_endpoint_existing_conversation PASSED [ 82%]
apps/core/tests/test_orchestrator.py::TestChatAPIWithOrchestrator::test_chat_endpoint_tracks_agent_and_llm PASSED [ 88%]
apps/core/tests/test_orchestrator.py::TestChatAPIWithOrchestrator::test_chat_endpoint_with_classify_trigger PASSED [ 94%]
apps/core/tests/test_orchestrator.py::TestChatAPIWithOrchestrator::test_chat_endpoint_with_tool_trigger PASSED [100%]

====================== 17 passed in 11.21s ======================
```

---

## Usage Examples

### Example 1: Simple Chat (No Tools)

**Request:**
```bash
curl -X POST http://localhost:8000/api/chat/ \
  -H "Content-Type: application/json" \
  -d '{
    "message": "Hello, how are you?",
    "agent_id": "data-protection-expert",
    "model_id": "dummy"
  }'
```

**Response:**
```json
{
  "conversation_id": "123e4567-e89b-12d3-a456-426614174000",
  "status": "completed",
  "messages": [
    {"role": "user", "content": "Hello, how are you?"},
    {
      "role": "assistant",
      "content": "ðŸ¤– **Dummy Response** from **Data Protection Expert** using **Dummy LLM**\n\nYou said: \"Hello, how are you?\"\n\nThis is a simulated response...",
      "agent": "Data Protection Expert",
      "llm": "Dummy LLM"
    }
  ],
  "tool_results": [],
  "protegrity_data": {...}
}
```

### Example 2: Chat with Redact Tool

**Request:**
```bash
curl -X POST http://localhost:8000/api/chat/ \
  -H "Content-Type: application/json" \
  -d '{
    "message": "My social security number is 123-45-6789",
    "agent_id": "data-protection-expert",
    "model_id": "dummy"
  }'
```

**Response:**
```json
{
  "conversation_id": "123e4567-e89b-12d3-a456-426614174000",
  "status": "completed",
  "messages": [
    {"role": "user", "content": "My social security number is 123-45-6789"},
    {
      "role": "assistant",
      "content": "ðŸ¤– **Dummy Response**...\n\nI detected sensitive data and will use Protegrity tools to protect it.\n\nTools requested: protegrity-redact\n\n---\n**Tools Used:**\n- âœ… protegrity-redact: Success",
      "agent": "Data Protection Expert",
      "llm": "Dummy LLM"
    }
  ],
  "tool_results": [
    {
      "call_id": "tool_call_1",
      "tool_name": "protegrity-redact",
      "output": {
        "redacted_text": "My social security number is [SSN]",
        "original_length": 40,
        "redacted_length": 35,
        "metadata": {"success": true, "method": "redact"}
      }
    }
  ],
  "protegrity_data": {...}
}
```

### Example 3: Multiple Tools

**Request:**
```bash
curl -X POST http://localhost:8000/api/chat/ \
  -H "Content-Type: application/json" \
  -d '{
    "message": "Please classify this text and check guardrails: My email is john@example.com",
    "agent_id": "data-protection-expert",
    "model_id": "dummy"
  }'
```

**Response:**
```json
{
  "conversation_id": "123e4567-e89b-12d3-a456-426614174000",
  "status": "completed",
  "tool_results": [
    {
      "call_id": "tool_call_1",
      "tool_name": "protegrity-classify",
      "output": {
        "entities": {
          "EMAIL_ADDRESS": [
            {"score": 0.99, "start_index": 19, "end_index": 35, "entity_text": "john@example.com"}
          ]
        },
        "entity_types": ["EMAIL_ADDRESS"],
        "total_entities": 1
      }
    },
    {
      "call_id": "tool_call_2",
      "tool_name": "protegrity-guardrails",
      "output": {
        "outcome": "accepted",
        "risk_score": 0.1,
        "policy_signals": []
      }
    }
  ],
  "messages": [...],
  "protegrity_data": {...}
}
```

### Example 4: Programmatic Usage

```python
from apps.core.orchestrator import ChatOrchestrator
from apps.core.models import Conversation, Message

# Get conversation
conversation = Conversation.objects.get(id="...")

# Create user message
user_message = Message.objects.create(
    conversation=conversation,
    role="user",
    content="My SSN is 123-45-6789"
)

# Use orchestrator
orchestrator = ChatOrchestrator()
result = orchestrator.handle_user_message(conversation, user_message)

# Access results
assistant_msg = result["assistant_message"]
tool_results = result["tool_results"]

print(f"Status: {result['status']}")
print(f"Assistant: {assistant_msg.content}")
print(f"Tools executed: {len(tool_results)}")

for tool_result in tool_results:
    print(f"  - {tool_result['tool_name']}: {tool_result.get('output', tool_result.get('error'))}")
```

---

## Future Enhancements

### 1. Multi-Step Tool Execution

Currently: **LLM â†’ Tools â†’ Save**  
Future: **LLM â†’ Tools â†’ LLM â†’ Save**

Allow LLM to see tool results and generate a refined response:

```python
# First pass
result1 = provider.send_message(conversation, history, agent=agent)
tool_results = execute_tool_calls(agent, result1.tool_calls)

# Second pass with tool results
history_with_tools = history + tool_results
result2 = provider.send_message(conversation, history_with_tools, agent=agent)

# Save final result
assistant_msg = Message.objects.create(..., content=result2.content)
```

### 2. Real LLM Provider Tool Calling

Implement function calling for production providers:

**FinAI Provider:**
```python
class FinAIProvider(BaseLLMProvider):
    def send_message(self, conversation, messages, agent=None):
        # Send with function definitions
        functions = [tool.function_schema for tool in agent.tools.all()]
        
        response = fin_ai_api.send_message(
            messages=messages,
            functions=functions
        )
        
        # Extract tool calls from response
        tool_calls = self._parse_fin_tool_calls(response)
        
        return ProviderResult(
            status="pending",
            pending_message_id=response.conversation_id,
            tool_calls=tool_calls
        )
```

**Bedrock Claude Provider:**
```python
class BedrockClaudeProvider(BaseLLMProvider):
    def send_message(self, conversation, messages, agent=None):
        # Use Claude's tool use feature
        tools = [self._convert_to_claude_tool(t) for t in agent.tools.all()]
        
        response = bedrock.invoke_model(
            modelId="anthropic.claude-3-sonnet-20240229",
            body={
                "messages": messages,
                "tools": tools
            }
        )
        
        # Parse tool_use blocks
        tool_calls = self._parse_claude_tool_calls(response)
        
        return ProviderResult(
            status="completed",
            content=response.content,
            tool_calls=tool_calls
        )
```

### 3. Custom Tool Types

Beyond Protegrity, support other tool types:

```python
# In tool_router.py
def execute_tool_calls(agent, tool_calls):
    for call in tool_calls:
        tool = agent_tools.get(call["tool_name"])
        
        if tool.tool_type == "protegrity":
            output = _execute_protegrity_tool(tool, args)
        elif tool.tool_type == "api":
            output = _execute_api_tool(tool, args)
        elif tool.tool_type == "database":
            output = _execute_database_tool(tool, args)
        elif tool.tool_type == "custom":
            output = _execute_custom_tool(tool, args)
```

**Example API Tool:**
```python
def _execute_api_tool(tool: Tool, args: Dict[str, Any]) -> Dict[str, Any]:
    """Execute external API tool"""
    schema = tool.function_schema
    
    response = requests.post(
        schema["endpoint"],
        json=args,
        headers={"Authorization": f"Bearer {schema['api_key']}"}
    )
    
    return response.json()
```

### 4. Tool Result Streaming

Stream tool execution progress to frontend:

```python
# In orchestrator
for tool_call in result.tool_calls:
    # Execute and stream result
    tool_result = execute_single_tool(agent, tool_call)
    
    # Send SSE event
    send_sse_event({
        "type": "tool_executed",
        "tool_name": tool_call["tool_name"],
        "result": tool_result
    })
```

### 5. Tool Execution Logging

Detailed logging for debugging and analytics:

```python
class ToolExecutionLog(models.Model):
    """Audit log for tool executions"""
    message = models.ForeignKey(Message, on_delete=models.CASCADE)
    tool = models.ForeignKey(Tool, on_delete=models.SET_NULL, null=True)
    call_id = models.CharField(max_length=100)
    arguments = models.JSONField()
    result = models.JSONField()
    execution_time_ms = models.IntegerField()
    success = models.BooleanField()
    error_message = models.TextField(null=True, blank=True)
    created_at = models.DateTimeField(auto_now_add=True)
```

### 6. Parallel Tool Execution

Execute independent tools concurrently:

```python
from concurrent.futures import ThreadPoolExecutor

def execute_tool_calls(agent, tool_calls):
    with ThreadPoolExecutor(max_workers=5) as executor:
        futures = [
            executor.submit(_execute_single_tool, agent, call)
            for call in tool_calls
        ]
        
        results = [f.result() for f in futures]
    
    return results
```

### 7. Tool Rate Limiting

Prevent abuse with rate limiting:

```python
from django.core.cache import cache

def execute_tool_calls(agent, tool_calls):
    for call in tool_calls:
        tool_name = call["tool_name"]
        
        # Check rate limit
        key = f"tool_limit:{agent.id}:{tool_name}"
        count = cache.get(key, 0)
        
        if count >= TOOL_RATE_LIMIT:
            results.append({
                "call_id": call["call_id"],
                "error": "Rate limit exceeded for this tool"
            })
            continue
        
        # Execute and increment
        result = _execute_tool(...)
        cache.set(key, count + 1, timeout=3600)
```

---

## Benefits

### 1. Separation of Concerns
- âœ… API endpoints focus on HTTP concerns
- âœ… Orchestrator handles business logic
- âœ… Tool router manages permissions and execution
- âœ… Providers focus on LLM communication

### 2. Security
- âœ… Agent-based tool permissions
- âœ… Active/inactive tool control
- âœ… No direct tool access from API
- âœ… All tool calls validated

### 3. Extensibility
- âœ… Easy to add new tool types
- âœ… Provider-agnostic tool calling
- âœ… Pluggable tool execution handlers
- âœ… Future: multi-step reasoning

### 4. Testability
- âœ… Unit tests for each component
- âœ… Integration tests for full flow
- âœ… Mock tools for testing
- âœ… 17 comprehensive tests

### 5. Observability
- âœ… Tool results stored in database
- âœ… Agent/LLM tracking on messages
- âœ… Detailed logging throughout
- âœ… Tool summary in assistant messages

---

## Migration from Task 2

### Before (Task 2)
```python
# views.py
provider = get_provider(llm)
result = provider.send_message(conversation, history, agent=agent)

if result.status == "completed":
    Message.objects.create(
        conversation=conversation,
        role="assistant",
        content=result.content,
        agent=agent,
        llm_provider=llm
    )
```

### After (Task 3)
```python
# views.py
orchestrator = ChatOrchestrator()
result = orchestrator.handle_user_message(conversation, user_message)

assistant_msg = result["assistant_message"]
tool_results = result["tool_results"]
```

**Changes Required:**
1. Import `ChatOrchestrator` instead of directly using providers
2. Call `orchestrator.handle_user_message()` instead of `provider.send_message()`
3. Access `assistant_message` and `tool_results` from result dict
4. No manual message creation needed (orchestrator handles it)

---

## Summary

Task 3 successfully implemented a **ChatOrchestrator** that:

1. âœ… Centralizes chat flow coordination
2. âœ… Routes messages to appropriate LLM providers
3. âœ… Executes tool calls with permission validation
4. âœ… Integrates Protegrity tools (redact, classify, guardrails)
5. âœ… Persists messages with agent/LLM tracking
6. âœ… Provides tool results to API consumers
7. âœ… Maintains security through agent-based permissions
8. âœ… Passes all 17 comprehensive tests

### Files Created
- `backend/apps/core/orchestrator.py` (265 lines)
- `backend/apps/core/tool_router.py` (230 lines)
- `backend/apps/core/tests/test_orchestrator.py` (520 lines)

### Files Modified
- `backend/apps/core/providers.py` - Extended `ProviderResult`, updated `DummyProvider`
- `backend/apps/core/views.py` - Refactored `/api/chat/` and `/api/chat/poll/`

### Test Results
```
17 tests passed âœ…
- 7 tool router tests
- 5 orchestrator tests  
- 5 API integration tests
Time: 11.21s
```

**Next Steps:**
- Implement real LLM provider tool calling (Fin AI, Bedrock)
- Add multi-step reasoning (LLM â†’ Tools â†’ LLM)
- Implement tool execution logging/analytics
- Add parallel tool execution
- Create frontend UI for tool results display
